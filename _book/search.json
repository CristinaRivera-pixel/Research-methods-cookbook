[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Research Methods Cookbook",
    "section": "",
    "text": "This is my cookbook for my Advanced Research Methods Class with Dr. Abdi. The purpose of this book is to have a reference for all the Advanced Research Methods I have learned in this class and give an overview of each method and an interpretation of the findings using each method. This is for my future self, or anybody who find this."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Method Explanation\nAbout the Data\nInterpretation of the Data\n\nEach of the data analysis graphs were rendered using R studio. The Interpretation of the Data will be based off graphics and given results. Thats it. Enjoy."
  },
  {
    "objectID": "method_exp.html",
    "href": "method_exp.html",
    "title": "2  PCA Basics",
    "section": "",
    "text": "PCA stands for principal component analysis."
  },
  {
    "objectID": "method_exp.html#what-does-pca-do",
    "href": "method_exp.html#what-does-pca-do",
    "title": "2  PCA Basics",
    "section": "2.1 What does PCA do?",
    "text": "2.1 What does PCA do?\n\nPCA reduces dimensions\nHelps us visualize the variables since we can’t see beyond 2 dimensions and its hard to visualize things in more then 3 dimensions\nReduces noise in the data"
  },
  {
    "objectID": "method_exp.html#what-is-pcas-goal-and-why",
    "href": "method_exp.html#what-is-pcas-goal-and-why",
    "title": "2  PCA Basics",
    "section": "2.2 What is PCA’s goal and why?",
    "text": "2.2 What is PCA’s goal and why?\n\nReduce Noise\nReducing Noise in your dataset can be useful when you want to focus on what is important. Its easier to visualize the data when you have less dimensions. Not all data carry the same weight.\nCompressing the dataset\nCompressing the dataset can make it easier and faster for machine learning applications or other types of analysis to go through the data\nHelps use Visualize our dataset\nEach component will have variables that will often correlate with each other. Running a PCA can help us see those variables that have high levels of correlation with other variables."
  },
  {
    "objectID": "method_exp.html#where-is-pca-used",
    "href": "method_exp.html#where-is-pca-used",
    "title": "2  PCA Basics",
    "section": "2.3 Where is PCA used?",
    "text": "2.3 Where is PCA used?\n\nIn machine learning applications\nPCA technique is particularly useful in processing data where multi-colinearity exists between the features/variables.\nWhen there are a lot of dimensions in your data\nCan be used for denoising and data compression"
  },
  {
    "objectID": "method_exp.html#pca-terms",
    "href": "method_exp.html#pca-terms",
    "title": "2  PCA Basics",
    "section": "2.4 PCA Terms",
    "text": "2.4 PCA Terms\nBelow are some terms that you will want to know before you jump in to the analysis.\n\nObservations\n\nThese will be composed of our variables and their values. Observations can be separated by time, by categories (for example individual people can have their own observations) or other factors.\n\nVariables\n\nPart of an observation, could be something like a rating for a magazine. Needs to be continous.\n\nStandardization and Mean centering\n\nA way of making sure that each variable is on the same scale as all the other variables. if one variable is on a larger scale then the larger variable value could over take its contribution to the components. For example think of housing prices and number of rooms. If they were standardized then housing prices would over be over represented while number of room would be under represented.\n\nComponents\n\nThese are the new variables which are linear combinations of the initial variables.\nComponent 1 is always the component with the most variance with each subsequent component having less variance then the previous but more variance then the components after. Each component is orthogonal to each other.\n\nCovariance\n\nHow two variables relate to each other . Can be positive, negative, or zero.\n\nCorrelation\n\nSimilar to covariance except it is restricted to a certain range.\n\nEigenvectors\n\ncontains eigenvalues from highest to lowest of all of variables.\n\nEigenvalues\n\nwe can think of eigen values as weights. It is important to keep eigenvectors with eigenvalues that contribute significantly to the components."
  },
  {
    "objectID": "method_exp.html#how-are-the-components-created",
    "href": "method_exp.html#how-are-the-components-created",
    "title": "2  PCA Basics",
    "section": "2.5 How are the Components created?",
    "text": "2.5 How are the Components created?\nThe components are created using the eigenvectors and eigenvalues calculated from the covariance heat matrix. One thing that happens when we are creating components is that we don’t use all of our eigenvectors which results in dimension reduction with some loss of accuracy. This loss in accuracy is always something to keep in mind as you will want to find the right balance of keeping accuracy loss low while not using the dimensions that don’t contribute much."
  },
  {
    "objectID": "about_data.html",
    "href": "about_data.html",
    "title": "3  The Data",
    "section": "",
    "text": "Our data:\n\n8 panelist\n7 different strawberry yogurts\n56 observations\n24 variables per observation"
  },
  {
    "objectID": "interpret_data.html",
    "href": "interpret_data.html",
    "title": "4  Interpreting the Data",
    "section": "",
    "text": "In this section I will be interpreting the Data"
  },
  {
    "objectID": "interpret_data.html#covariance-heat-map",
    "href": "interpret_data.html#covariance-heat-map",
    "title": "4  Interpreting the Data",
    "section": "4.1 Covariance Heat map",
    "text": "4.1 Covariance Heat map\nFirst, lets take a look at our covariance heat map of our strawberry yogurts and there sensory inputs. Our covariance heat map is great for looking at our variables at a glance to see which ones may have a larger/smaller effect on our components.\n\n\n\nCovariance Heat Map\n\n\nFirst, we will look at some of the variances on the covariance heat map, which include all our variables on the left-hand side. Note that all the variable’s variances are positive since they are not negative. One thing to note is that the larger the variance, the more the variable will co-vary, indicating it will be an essential variable. So our top variance is Fruity with .43. This one may be one to look out for (or maybe not)."
  },
  {
    "objectID": "interpret_data.html#correlation-heat-map",
    "href": "interpret_data.html#correlation-heat-map",
    "title": "4  Interpreting the Data",
    "section": "4.2 Correlation Heat map",
    "text": "4.2 Correlation Heat map\nNext, lets take a look at some of our correlations.\n\nFor a PCA, it is essential to look at our correlations; after all, if two variables are highly correlated with each other, then it is time to take a look at whether it would be advantageous to weigh them equally. In this data, the most notable correlations are:\n\n4.2.1 Top Correlations:\n\nSweet and Cheesy -71\nAnthranalite and Grape 71\n\nNote: Anthranalite is actually an artifical grape flavoring that is used in products like grape kool-aid so it makes sense that they have one of the strongest correlations!\n\nAnthranalite and Cheesy -62\nCheesy and Lactonic 61\n\nNote: Lactonic in this context is a “milky” type of flavor. So, again, it makes sense that they are highly correlated with each other\n\nBitter and Cheesy 60\nBitter and Jammy/Raspberry -60\nAstringent and Caramelic -59\n\nAnother thing to note when looking at this graph is how many variables have high correlations with other variables. For example, take a look at the “sweet” column, and you can notice that it has about six variables that have a strong correlation with it (Pearson correlation>.5). This could mean many things. If there are many positive correlations, those correlations mean that the variables represent similar things. The negative correlations mean that they represent the same thing also, but in a different direction. For example, our Sweet and Cheesy variables have a correlation of -71, representing a strong negative correlation with each other. This correlation means sweet yogurts are not typically Cheesy and vice versa."
  },
  {
    "objectID": "interpret_data.html#explained-variance-scree-plot",
    "href": "interpret_data.html#explained-variance-scree-plot",
    "title": "4  Interpreting the Data",
    "section": "4.3 Explained Variance Scree plot",
    "text": "4.3 Explained Variance Scree plot\nCalculating our eigenvalues and dividing each by the cumulative eigenvalues will give us our percentage of explained variance. We will want to have the maximum explained variance while reducing our dimensions. To do this, we will look at a Scree Plot that will plot our dimensions, aka eigenvalues, aka components, to their corresponding explained variance percentage.\n\nExplained Variance Percentage\n\nThis is our Eigenvalue / Cumulutive Sum of Eigenvalues which will represent exactly how much variance this eigenvalue or component brings.\n\nScree Plot\n\nA line plot of the cumulative eigenvalues of the Explained Variance.\n\n\n\nOnce we have calculated our components, aka dimensions, we will want to know how many components we should keep in our analysis. There are two different tests that I will be talking about that we can use here (although there are probably other ways to do this analysis!).\n\nElbow test\nThe “elbow” is where the percentage of Explained Variance only makes marginal gains. Typically, it will be the “bend” of the elbow when you look at the graph. In this plot, we can see that the elbow starts around dimension 3. There are two ways to interpret this. The first is to keep all the components at the bend. The second option is to keep the components before the bend. If we keep all the components at the bend, then we would keep three components. If we keep before the bend, then we would keep two components.\nKaisar Criterion\nInstead of using the elbow test, which often produces too few components and is more subjective, use the Kaisar Criterion, which is what the purple line above represents. The Kaisar Line is created by taking the average of all the explained variances and then plotting that line onto the graph. So, everything above the line, keep, and below the line, do not keep."
  },
  {
    "objectID": "interpret_data.html#contributions",
    "href": "interpret_data.html#contributions",
    "title": "4  Interpreting the Data",
    "section": "4.4 Contributions",
    "text": "4.4 Contributions\nNext, we will take a look at our contributions. For this, we have two different factors: our observations and our variables. Specific observations can make more significant contributions than other observations (same thing with the variables!), which simultaneously means they contribute a lot to the component and that they are more likely to be better represented by our components.\n\n4.4.1 Observations\n\nIn this bar plot, each different color represents a different yogurt. For example the lime green bars represent the contribution from each observation of the yogurt Alpura Frutal. Anything over the line represents contribution that is larger then the average of the contribution and is a notable contribution. In this graph all the yo-plait Greek yogurt (in yellow) seems to make the smallest contribution to component 1.\n\n\n\nThis bar plot for component 2 has one very notable observation that seems to have made a overwhelmingly large contribution to component 2.\n\n\n\n\n4.4.2 Variables\nWhen looking at contribution it is also important to look at how much each of the variables contributed to our components.\n\nFor component 1 our largest contributor to this component was Sweet and Cheesy which also happen to be 1 of 2 of our strongest correlation. There are also other variables that have similar levels of contribution. Most of these variables are correlated with each other but it will be easier to see it on the correlation circle then here.\n\nIn this graph, Overall flavor and Overall aroma seemed to be the biggest contributors then Jammy/Strawberry and Ripe."
  },
  {
    "objectID": "interpret_data.html#correlation-circle",
    "href": "interpret_data.html#correlation-circle",
    "title": "4  Interpreting the Data",
    "section": "4.5 Correlation Circle",
    "text": "4.5 Correlation Circle\nLets take a look at our correlation circle and make some obervations about it.\n\nOur correlation circle here has a few things plotted . We have our observations that is plotted using the colorful circles, and then we have ours variables plotted as well each with their corresponding correlation score to our components. For example, We can notice the lime green clusters on the right side are best represented by component one and a little by component 2. Then we can also notice within those green clusters, the variables grape and anthranalite pop up, which means that these this yogurt may be one that is more of a grape/anthranalite flavor.\nAnother element we have on our graph is our correlation circle itself, which will help us determine how well our observations and variables are represented by our data.\nFirst thing I notice when I look at this correlation circle is that none of our variables touch the circle. This means that their variance is not explained perfectly by our components. We do have some components that are a bit close to the circle such as sweet and cheesy. Since we have none that are on the circle itself we have to take some precautions when making a statement about the variables correlation with each. Here are some general guidelines for the angles between two variables:\n\n\n\nDegree of Angle\nCorrelation\n\n\n\n\naround 180 degrees\nNegative correlation\n\n\naround 90 degrees\nNo correlation or very weak correlation\n\n\nless then 90 degrees\nPositive correlation\n\n\n\nSince none of our variables touch the circle however Its important to point out that the angles may not be entirely accurate. For example since sweet and buttery are at 180 degrees you may assume that they are negatively correlated but since buttery is not represented well by either of our components (which we can tell because it is closer to the center of our graph) we cannot trust the angles. For variables like sweet and cheesy however, you can make this connection and they do seem to be negatively correlated with each , with even our correlation heat map making this connection.\nOne thing to note, since our components don’t represent salty and buttery well this could mean they are probably better represented by a different component. Perhaps if we take a look at component three or beyond they are better represented in those."
  },
  {
    "objectID": "interpret_data.html#confidence-and-tolerance-interval",
    "href": "interpret_data.html#confidence-and-tolerance-interval",
    "title": "4  Interpreting the Data",
    "section": "4.6 Confidence and Tolerance Interval",
    "text": "4.6 Confidence and Tolerance Interval\n\n4.6.1 Tolerance Intervals\n\n\n\nTolerance Intervals with mean factor Scores\n\n\nOur Tolerance Intervals Gives is our maximum surfuce area of the points along with our mean factor scores which are illustrated by the triangles in the middle of the areas. This graph is great for illustrating the spread of the points and also showing us how our points compare to the average of the points. For example the Lala yogurt has a outliar observation that doesn’t seem to be anywhere close to the mean factor score for it.\n\n\n4.6.2 Confidence Intervals\n\nWhen analyzing components its important to think about the accuracy of the components. These confidence intervals represent the accuracy of our components to generalizing our yogurts. The larger the surface area of the yogurt, the less accurate we can that the component will represent for that yogurt. In this instance, we have a LaLa yogurt which seems to have a low confidence intervals. In contrast, our Danup yogurt will have a high degree of accuracy."
  }
]